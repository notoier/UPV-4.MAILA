{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mK5XAIf9jga"
      },
      "source": [
        "#Training Neural Networks\n",
        "In this lab session, we will practice the training techniques and recipes seen during the theoretical sessions. Remember that there are three main aspects you have to consider:\n",
        "\n",
        "\n",
        "1.   One time setup considerations.\n",
        "2.   Improve your training error.\n",
        "3.   Improve your test error.\n",
        "\n",
        "We will stick ourselves to a fixed MLP architecture and see how different training techniques impact our results, both in train and development. Finally, using all the lessons learned, we will train our MLP and check the results in the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRhTJGkRAcFt"
      },
      "source": [
        "##The dataset\n",
        "We will work with a dataset of images called [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). In this dataset, we have images of size $32 \\times 32 \\times 3$, corresponding to height (H), width (W) and channels (C). Remember that coloured images use 3 channels (red-green-blue).\n",
        "\n",
        "All the images of the dataset are classified into 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. So, our task is to build a neural network that classifies images correctly. But first of all, let's download the dataset to prepare it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLW8N0ib6KI4"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "print(f'Dataset samples: {len(training_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlAxm5BJCPMc"
      },
      "source": [
        "We have only downloaded training images, to have a dataset of 50 thousand images. Remember that test images cannot be used during the design and development of our neural network. However, to guide our decisions regarding hyperparameters, we will generate a **development set**. The best way to do that is to use a **stratified** partition, i.e. a random partition where we maintain the original distribution of classes. For that purpose, we use the `train_test_split()` function of scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iz9gMmeG-MO"
      },
      "source": [
        "# Split the indices in a stratified way\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "indices = np.arange(len(training_data))\n",
        "train_indices, dev_indices = train_test_split(indices, train_size=40000, stratify=training_data.targets, random_state=42)\n",
        "\n",
        "train_set = Subset(training_data, train_indices)\n",
        "dev_set = Subset(training_data, dev_indices)\n",
        "\n",
        "print(f'Train samples: {len(train_set)}')\n",
        "print(f'Dev samples: {len(dev_set)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8C6-qJqFrx0"
      },
      "source": [
        "Check whether both sets are balanced in terms of the number of samples for each class. The following cell calculates the percentage of samples of each set for each of the labels. Remember that train and dev sets should follow the same distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoxXLtgfO2ZB"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "train_counter = Counter(list(zip(*train_set))[1])\n",
        "train_sorted = sorted(train_counter.items(), key = lambda kv: kv[0])\n",
        "train_sorted_labels = list(zip(*train_sorted))[0]\n",
        "train_percentages = np.array(list(zip(*train_sorted))[1]) / len(train_set)\n",
        "\n",
        "dev_counter = Counter(list(zip(*dev_set))[1])\n",
        "dev_sorted = sorted(dev_counter.items(), key = lambda kv: kv[0])\n",
        "dev_sorted_labels = list(zip(*dev_sorted))[0]\n",
        "dev_percentages = np.array(list(zip(*dev_sorted))[1]) / len(dev_set)\n",
        "\n",
        "print(f\"Train (label, %): {list(zip(train_sorted_labels, train_percentages))}\")\n",
        "print(f\"Dev (label, %): {list(zip(dev_sorted_labels, dev_percentages))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqlMtnJ41YQL"
      },
      "source": [
        "As can be seen, both sets are perfectly balanced: each class represents the 10% of both sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ6CCkHJBOhC"
      },
      "source": [
        "We can have a look at the images of the train set, to have an idea of what kind of problem we are facing.\n",
        "\n",
        "**NOTE:** When using images in Pytorch, the tensors are arranged as (C, H, W), instead of the more common (H, W, C) format. Take that into account, specially for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tN6ctN-D7PJ"
      },
      "source": [
        "labels_map = {\n",
        "    0: \"airplane\",\n",
        "    1: \"automobile\",\n",
        "    2: \"bird\",\n",
        "    3: \"cat\",\n",
        "    4: \"deer\",\n",
        "    5: \"dog\",\n",
        "    6: \"frog\",\n",
        "    7: \"horse\",\n",
        "    8: \"ship\",\n",
        "    9: \"truck\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_set), size=(1,)).item()\n",
        "    img, label = train_set[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.permute(1, 2, 0)) # Permute since Pytorch uses (C, H, W) and plt needs (H, W, C)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcrwzkREKdyK"
      },
      "source": [
        "##The neural network\n",
        "We will use a multi-layer perceptron (MLP) for this task. As we said before, we will not change the architecture of the model, as we are not interested on architectural decisions for now (how many layers? How many neurons in each layer?).\n",
        "\n",
        "**EXERCISE:** Design a multi-layer perceptron (MLP) with the following architecture: INPUT -> LINEAR(512) -> ReLU -> LINEAR(512) -> ReLU -> LINEAR(10)\n",
        "\n",
        "**NOTE:** Remember that images are tensors of shape (3, 32, 32) and the input for a linear layer should be a vector (in this case of size $3\\times 32 \\times 32 = 3072$).\n",
        "\n",
        "**TODO:** Decide where and how to explain the issue about the absence of an activation function in the output layer and the Pytorch CrossEntropyLoss (softmax + NLL loss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPIT-AnqWiM"
      },
      "source": [
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class MyMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(MyMLP, self).__init__()\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 5 lines)\n",
        "\n",
        "      ########################\n",
        "\n",
        "    def forward(self, x):\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 6 lines)\n",
        "\n",
        "      ########################\n",
        "      return logits\n",
        "\n",
        "model = MyMLP()\n",
        "\n",
        "# Print the number of trainable parameters\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O5SA17qBfvD"
      },
      "source": [
        "Your MLP should have 1,841,162 learnable parameters, i.e. slightly above 1.8M parameters.\n",
        "\n",
        "During the set-up of a neural network, many training processes have to be performed, so training time becomes an essential part of the problem. To speed-up our training processes, we will use a GPU availabe in Colab. The following cell assigns `cuda` to the variable `device` if a GPU is available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zb7wuFkrl_g"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg7g2Y6cBj-_"
      },
      "source": [
        "We will also define the base train and test functions to use through the lab. Here you have the implementation of the functions `train_loop()` and `test_loop()`. Some details to highlight here:\n",
        "\n",
        "1.   Notice how we use `model.train()` and `model.eval()`. This is very important. As model behaviour during training and testing may be different (due to batch normalization and dropout, for example), Pytorch needs to know the \"mode\" in which we are using the model. It is also important as Pytorch does not store the gradients in `eval` mode, making calculations faster. Thus, when training, set `model.train()` and when testing, set `model.eval()`.\n",
        "2.   The function `test_loop` will be used for both development and test data. The process is the same for both datasets. The only thing that changes is the dataset itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MDSOm8K93lw"
      },
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      # Compute prediction and loss\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Store loss and accuracy\n",
        "      train_loss += loss.item()\n",
        "      train_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        loss, current = loss.item(), batch * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    train_acc /= size\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "          X = X.to(device)\n",
        "          y = y.to(device)\n",
        "          pred = model(X)\n",
        "          test_loss += loss_fn(pred, y).item()\n",
        "          test_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    test_acc /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5cqFPci-9xy"
      },
      "source": [
        "##One time setup considerations\n",
        "During the theoretical sessions, we saw that this step is to decide about activation functions, data preprocessing, weight initialization and batch normalization. Given the time constraints we have for the lab, we will not explore different activation functions. ReLU is our choice for all the layers. Similarly, regarding data preprocessing, we wil only scale the pixel values of the images from [0, 255] to [0, 1], and discard futher exploration. But we will skim through weight initialization and batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ddim5r-dSZ_"
      },
      "source": [
        "###Weight initialization\n",
        "When we use `torch.nn.Linear()`, Pytorch uses Xavier initialization by default. Hence, our `MyMLP` model is configured to use that initialization. Let's train it for 10 epochs to see the results we obtain in train.\n",
        "\n",
        "**EXERCISE:** Fill in the gaps following the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55mSeTJydU_M"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "# TODO: instantiate a MLP object in the model variable\n",
        "\n",
        "########################\n",
        "model = model.to(device)\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 2 lines)\n",
        "# TODO: create train_dataloader\n",
        "\n",
        "########################\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 2 lines)\n",
        "# TODO: instantiate a cross-entropy loss in the loss_fn variable\n",
        "# TODO: instantiate a SGD optimizer in the optimizer variable\n",
        "\n",
        "########################\n",
        "\n",
        "train_losses, train_accuracies  = [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6jy6Y6rVi3C"
      },
      "source": [
        "Our training accuracy is 0.299, very close to 0.3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2SovdlgeJ7M"
      },
      "source": [
        "We know from our theoretical sessions that when using ReLU activation function, He or Kaiming initialization should work better. Let's test it. We will implement a new class called `MyMLPKaiming` to apply He initialization to its linear layers. But we will keep the initial MLP architecture unchanged.\n",
        "\n",
        "**EXERCISE:** use `torch.nn.init.kaiming_uniform_` to re-initialize the weights of the linear layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u-zJ_I9dUvN"
      },
      "source": [
        "class MyMLPKaiming(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(MyMLPKaiming, self).__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear1 = nn.Linear(3*32*32, 512)\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "      ########################\n",
        "      self.linear2 = nn.Linear(512, 512)\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "      ########################\n",
        "      self.linear3 = nn.Linear(512, 10)\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "      ########################\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 6 lines)\n",
        "\n",
        "      ########################\n",
        "      return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su92aJHJq8td"
      },
      "source": [
        "We will train the new model just as before, to compare the results fairly.\n",
        "\n",
        "**EXERCISE:** Fill in the gaps following the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az6H9xRNed0p"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "# TODO: instantiate a MLPKaiming object in the model variable\n",
        "\n",
        "########################\n",
        "model = model.to(device)\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 2 lines)\n",
        "# TODO: create train_dataloader\n",
        "\n",
        "########################\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 2 lines)\n",
        "# TODO: instantiate a cross-entropy loss in the loss_fn variable\n",
        "# TODO: instantiate a SGD optimizer in the optimizer variable\n",
        "\n",
        "########################\n",
        "\n",
        "train_losses, train_accuracies  = [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndoq8dlUfKAE"
      },
      "source": [
        "This is quite a big improvement! When linear layers were initialized with Xavier initialization, we had a training accuracy of 0.29. Now, with He initialization, we achieve 0.40. So far, we have seen that using He initialization our optimization improves faster in 10 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFnx8EtPhl0a"
      },
      "source": [
        "###Batch normalization\n",
        "To implement batch normalization, we have to create another class. We will name it `MyMLPBN`. Remember that batch normalization normalizes activations with learned means and variances on mini-batches.\n",
        "\n",
        "**EXERCISE:** implement the necessary code to apply batch normalization to our MLP, both in the `__init__` and `forward` functions. Create 2 batch normalization layers, as much as hidden layers we have in the network, and apply them correctly in the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWmNt1VvhpAo"
      },
      "source": [
        "class MyMLPBN(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(MyMLPBN, self).__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear1 = nn.Linear(3*32*32, 512)\n",
        "      torch.nn.init.kaiming_uniform_(self.linear1.weight)\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "      ########################\n",
        "      self.linear2 = nn.Linear(512, 512)\n",
        "      torch.nn.init.kaiming_uniform_(self.linear2.weight)\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "      ########################\n",
        "      self.linear3 = nn.Linear(512, 10)\n",
        "      torch.nn.init.kaiming_uniform_(self.linear3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 8 lines)\n",
        "\n",
        "      ########################\n",
        "      return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrs8wG0x1YDf"
      },
      "source": [
        "We are ready to train this new model with batch normalization. Again, we will use the same setting as the previous experiments, to fairly compare the obtained results.\n",
        "\n",
        "**EXERCISE:** fill in the gaps following the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygVTfYXbip0D"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "# TODO: instantiate a MyMLPBN object in the variable model\n",
        "\n",
        "########################\n",
        "model = model.to(device)\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "# TODO: create train_dataloader\n",
        "\n",
        "########################\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 2 lines)\n",
        "# TODO: instantiate a cross-entropy loss in the loss_fn variable\n",
        "# TODO: instantiate a SGD optimizer in the optimizer variable\n",
        "\n",
        "########################\n",
        "\n",
        "train_losses, train_accuracies  = [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ihZDtZjjDcK"
      },
      "source": [
        "We got another big boost here! Train accuracy is around 0.48, showing that batch normalization helps a lot in the training process. Notice that batch normalization usually allows increasing the learning rate to speed-up optimization, so let's try it. We will increase the learning rate ten times to check what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4lkG6KK-fZ_"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-2 # New learning rate\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = MyMLPBN()\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_losses, train_accuracies  = [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNJBl09O39dS"
      },
      "source": [
        "Just increasing the learning rate to $10^{-2}$, we obtain 0.67 of accuracy in train. As can be seen, the impact of hyperparameters such as learning rate and techniques such as batch normalization can be very significant in the training process of a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4FfCSz1td3i"
      },
      "source": [
        "##Improve your training error\n",
        "Now that one time setup considerations are clear for us, we will explore optimizers and schedulers to improve our training performance even more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkHuJ_OM6MH0"
      },
      "source": [
        "###SGD with momentum\n",
        "Keeping the same configuration as before, we will test how SGD with momentum works.\n",
        "\n",
        "**EXERCISE:** Substitute the SGD optimizer by SGD with momentum and use a momentum value of 0.9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x4sHG7O6UEk"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = MyMLPBN()\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "########################\n",
        "\n",
        "train_losses, train_accuracies  = [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht7xElaC6mLj"
      },
      "source": [
        "We keep on improving! You should have an accuracy of 0.73, which shows that SGD with momentum steps faster towards the global minimum than vanilla mini-batch gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU3qmRw57c8N"
      },
      "source": [
        "###Adam\n",
        "We will skip RMSProp and use the Adam optimizer directly, which is usually the *de facto* standard in deep learning. Adam usually benefits from smaller learning rates, so we will decrease it to $10^{-4}$.\n",
        "\n",
        "**EXERCISE:** use Adam with default parameter values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CywjelPu7en0"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-4 # Decrease the learning rate\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = MyMLPBN()\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "########################\n",
        "\n",
        "train_losses, train_accuracies  = [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2hs5gHu7zfM"
      },
      "source": [
        "Well, we obtain a very similar train accuracy with Adam, i.e. around 0.73. Although it would be better to explore learning rate values more exhaustively (random search), let's stick to Adam with lr=1e-4 and train for more epochs, as the training accuracy is still improving. In the following cell you have the code to train our model for **30 epochs more** with the Adam optimizer, up to a total of 40 epochs. Look carefully at the code: as the state of `model` and `optimizer` are still stored in the session of Colab, we can resume the training process from the last epoch (epoch number 10)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0UzwRxB_L76"
      },
      "source": [
        "import time\n",
        "\n",
        "start_epoch = 10 # Resume from the last training point\n",
        "epochs = 30 # Add more epochs to our training\n",
        "start = time.time()\n",
        "for t in range(start_epoch, start_epoch+epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fERgMcK-IXI"
      },
      "source": [
        "Wow! We achieve **0.96 accuracy** in training (very-very close to 0.97). Let me remember that the frist trial we made with the same neural network architecture achieved 0.29 accuracy. Thus, changing weight initialization, including batch normalization, tweaking a bit the learning rate, using Adam and increasing the epochs, we have now 0.96 accuracy. So **deep learning is not just about adding layers and neurons**. There are many techniques and approaches that can dramatically change the behaviour of the same neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snFdhPmN-zlS"
      },
      "source": [
        "To have a better idea of how the training process went, let's look at training learning curves for loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq_WcKEjAz1C"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = start_epoch + epochs\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(epochs), train_losses, color='blue', label='train')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(epochs), train_accuracies, color='blue', label='train')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxqPsFm8Ami-"
      },
      "source": [
        "It seems those final epochs were a little bit oscillating, but our overall performance is great already."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG-ZH_GrBBB9"
      },
      "source": [
        "###Learning rate schedulers\n",
        "From the learning curves above, we can guess the optimizer might be wandering around the global minimum. We could improve a little bit more if we use some learning rate scheduler. For that, we have to change our training function. Notice that now we pass a `scheduler` to our new function. In the body of the function, it is enough to add a `scheduler.step()` call in the suitable position, just after processing a complete epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tivLC3HrBTup"
      },
      "source": [
        "def train_loop_scheduler(dataloader, model, loss_fn, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      # Compute prediction and loss\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Store loss and accuracy\n",
        "      train_loss += loss.item()\n",
        "      train_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        loss, current = loss.item(), batch * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    scheduler.step() # Calculate the new learning rate for the next epoch\n",
        "    train_loss /= num_batches\n",
        "    train_acc /= size\n",
        "    return train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPZCjchICBRV"
      },
      "source": [
        "Now we can use any scheduler from the [long list of Pytorch](https://pytorch.org/docs/stable/optim.html). For this example, we will use the cosine scheduler.\n",
        "\n",
        "**EXERCISE:** write the code to use a cosine scheduler with our new function `train_loop_scheduler()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z7ZBafoCKrG"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "epochs = 40\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = MyMLPBN()\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "########################\n",
        "\n",
        "train_losses, train_accuracies  = [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    ### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "    # TODO: call to the new training function with the scheduler and store the returned values\n",
        "\n",
        "    ########################\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR0I_HJwEWx2"
      },
      "source": [
        "The train accuracy improved further with a best value of 0.983. Let's look at the training curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gh-okj-EZSP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(epochs), train_losses, color='blue', label='train')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(epochs), train_accuracies, color='blue', label='train')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiPQhF8QF3Qc"
      },
      "source": [
        "We will stop here, as we already have a very good training regime. To sum up, this is our current configuration:\n",
        "\n",
        "\n",
        "*   Data preprocessing: scale pixel values between 0 and 1 (we did not explore this).\n",
        "*   Weight initialization: He.\n",
        "*   Batch normalization: yes.\n",
        "*   Batch size: 64 (we did not explore this hyperparameter).\n",
        "*   Learning rate (initial): 1e-4.\n",
        "*   Epochs: 40.\n",
        "*   Optimizer: Adam.\n",
        "*   Scheduler: Cosine (we did not test other schedulers).\n",
        "\n",
        "Our best training accuracy is 0.98.\n",
        "\n",
        "Let's save our best trained model in the disk, just in case.\n",
        "\n",
        "**EXERCISE:** write the code to save the trained model under the name of *model.pth*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6mbnoEyILcd"
      },
      "source": [
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAY4OBGvGqt1"
      },
      "source": [
        "##Improve your test error\n",
        "Although our training accuracy is very good and we should be satisfied, the most important performance metric is accuracy for new, unseen samples. We will estimate such performance using our development set.\n",
        "\n",
        "**EXERCISE:** write the code to load the previously saved Pytorch model called *model.pth* into the `model` variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8y_Kpq5Npjy"
      },
      "source": [
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37dx8nE6DTFe"
      },
      "source": [
        "**EXERCISE:** write the code to create `dev_dataloader` from `dev_set`. Do not shuffle the data for development, as it is only to check the performance. Write also de code to call to `test_loop` and obtain `dev_loss` and `dev_acc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ1fUFIuNvrJ"
      },
      "source": [
        "### WRITE YOUR CODE HERE ### (≈ 2 lines)\n",
        "\n",
        "########################\n",
        "\n",
        "print(f'dev loss: {dev_loss}, dev accuracy: {dev_acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgaESpfGPPZy"
      },
      "source": [
        "Our model is clearly overfitting! It obtains 0.54 accuracy for new data, when training accuracy was 0.98. We need some regularization here. But first of all, we have to check learning curves also for development data, to have a better diagnose. To reduce training time, we will only train for 20 epochs, as it is enough to have a clear idea of what is going on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R63AaqbuQeW0"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = MyMLPBN()\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "train_losses, train_accuracies, dev_accuracies, dev_losses  = [], [], [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop_scheduler(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    dev_loss, dev_acc = test_loop(dev_dataloader, model, loss_fn, device)\n",
        "    dev_losses.append(dev_loss)\n",
        "    dev_accuracies.append(dev_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "print(f'Best dev accuracy: {max(dev_accuracies)} in epoch {dev_accuracies.index(max(dev_accuracies))+1}')\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRGJxq13RcLv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(epochs), train_losses, color='blue', label='train')\n",
        "plt.plot(range(epochs), dev_losses, color='green', label='dev')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(epochs), train_accuracies, color='blue', label='train')\n",
        "plt.plot(range(epochs), dev_accuracies, color='green', label='dev')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujhgkuaBVpbj"
      },
      "source": [
        "The development performance plateaus very early in the training process. That means that the features learned during the training phase do not help for development (new) data. Our network is learning irrelevant features for the problem. We have to attack that problem now, and our weapons are regularization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDNKbYi5HQZ9"
      },
      "source": [
        "###Data augmentation\n",
        "Data augmentation is always a good idea and it is specially well suited for images. In Pytorch, using the torchvision library, it is remarkably easy to implement data augmentation pipelines. You can have a look [here](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py) to see the available transformations and their effects.\n",
        "\n",
        "**EXERCISE:** Using `transforms.Compose`, build the data augmentation pipeline in the variable `our_transforms`. Use the following transforms for the pipeline: random horizontal flip (probability of 0.5), color jitter (default parameters), random crop (size $28 \\times 28$) and random invert (probability of 0.5).\n",
        "\n",
        "**NOTE FOR THE EXERCISE:** after applying a random crop, your image will become of size $28 \\times 28$. Check what you should do to restore its previous size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDtEZDiZH2cS"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "### WRITE YOUR CODE HERE ### (≈ 2 lines)\n",
        "\n",
        "########################\n",
        "\n",
        "# Add the transforms to our dataset\n",
        "# NOTE: From now on, train_set will always have these transforms\n",
        "train_set.dataset.transform = our_transforms\n",
        "\n",
        "# Let's visualize some images\n",
        "train_dataloader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "imgs, labels = next(iter(train_dataloader)) # We have a batch of 4 images and labels\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(imgs)\n",
        "\n",
        "imshow(out, title=[labels_map[x.item()] for x in labels])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phoeGJM74K8D"
      },
      "source": [
        "Data augmentation, and all the other regularization techniques, make training more difficult, so we will need more epochs to obtain good results. However, increasing the number of epochs we also increase the training time, which is a problem for this lab. For that reason, we will restrict ourselves to 20 epochs of training and try to gain intuition looking at learning curves, and comparing regularization techniques on this basis. So, please, analyse the obtained results into this context. And be patient, since the following experiments needs around **6 minutes** to be completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJyS_xyrSTBj"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "epochs = 20 # Restrict epochs to reduce training time\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = MyMLPBN()\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "train_losses, train_accuracies, dev_accuracies, dev_losses  = [], [], [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop_scheduler(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    dev_loss, dev_acc = test_loop(dev_dataloader, model, loss_fn, device)\n",
        "    dev_losses.append(dev_loss)\n",
        "    dev_accuracies.append(dev_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "\n",
        "print(f'Best dev accuracy: {max(dev_accuracies)} in epoch {dev_accuracies.index(max(dev_accuracies))+1}')\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HusEPygXdBrv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(epochs), train_losses, color='blue', label='train')\n",
        "plt.plot(range(epochs), dev_losses, color='green', label='dev')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(epochs), train_accuracies, color='blue', label='train')\n",
        "plt.plot(range(epochs), dev_accuracies, color='green', label='dev')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFlA2kGSu_d5"
      },
      "source": [
        "Learning curves are completely different now. Training and dev metrics are going together, which means that training performance is transferring to development performance. However, as can be seen, the best training accuracy (\\~0.50) is very far from the accuracies we obtained without data augmentation (\\~0.98). That means that training is now much harder, as the model is processing different images in each epoch. But our objective is to improve the performance in new images, i.e. dev accuracy. The best dev accuracy is 0.50, close to what we obtained with the previous model. Take into account that we trained only for 20 epochs now. And if we look at the curves, it seems our model could keep improving. Also you can try different transformations, besides the ones used here, and see what combination works better for this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsp8HEa3HPPp"
      },
      "source": [
        "###Weight decay\n",
        "Another regularization technique we saw is **weight decay**. More concretely, we will use $L2$ regularization, specifying a value for the hyperparameter $\\lambda$.\n",
        "\n",
        "**EXERCISE:** Modify the code below to include weight decay in our training process (expected training time around **6 minutes**).\n",
        "\n",
        "**HINT:** In Pytorch, weight decay has to be added in the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-C8u3TyV93P"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "weight_decay = 1e-3\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = MyMLPBN()\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "### WRITE YOUR CODE HERE ### (≈ 1 line)\n",
        "\n",
        "########################\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "train_losses, train_accuracies, dev_accuracies, dev_losses  = [], [], [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop_scheduler(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    dev_loss, dev_acc = test_loop(dev_dataloader, model, loss_fn, device)\n",
        "    dev_losses.append(dev_loss)\n",
        "    dev_accuracies.append(dev_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "\n",
        "print(f'Best dev accuracy: {max(dev_accuracies)} in epoch {dev_accuracies.index(max(dev_accuracies))+1}')\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPWPgXgy6gcO"
      },
      "source": [
        "Using only data augmentation, we obtained 0.5 best dev accuracy. Now, we get 0.504, which is a slight improvement. Different values for $\\lambda$ should be tested, using random search, for example. However, I would not expect big improvements with weight decay in this specific problem. In any case, let's see the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ_gw3suaZZC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(epochs), train_losses, color='blue', label='train')\n",
        "plt.plot(range(epochs), dev_losses, color='green', label='dev')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(epochs), train_accuracies, color='blue', label='train')\n",
        "plt.plot(range(epochs), dev_accuracies, color='green', label='dev')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1cc9S8glouA"
      },
      "source": [
        "Again, we can see that train and dev metrics are going together, which is  good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qtTymXcpRcK"
      },
      "source": [
        "###Dropout\n",
        "We will try to use dropout. There are still some [discussions](https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout) regarding batch normalization and dropout. It is not clear whether they should be used together and how. Here, we will try to combine both techniques and see how it performs.\n",
        "\n",
        "**EXERCISE:** Implement the `forward` dunction of the new class `MyMLPBNDrop` combining the usage of batch normalization and dropout. We added `dropout` as argument in the function `__init__` to specify the dropout probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iNIXzzfpnpp"
      },
      "source": [
        "class MyMLPBNDrop(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "      super(MyMLPBNDrop, self).__init__()\n",
        "      self.p = dropout\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear1 = nn.Linear(3*32*32, 512)\n",
        "      torch.nn.init.kaiming_uniform_(self.linear1.weight)\n",
        "      self.bn1 = nn.BatchNorm1d(num_features=512)\n",
        "      self.linear2 = nn.Linear(512, 512)\n",
        "      torch.nn.init.kaiming_uniform_(self.linear2.weight)\n",
        "      self.bn2 = nn.BatchNorm1d(num_features=512)\n",
        "      self.linear3 = nn.Linear(512, 10)\n",
        "      torch.nn.init.kaiming_uniform_(self.linear3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "      ### WRITE YOUR CODE HERE ### (≈ 10 lines)\n",
        "\n",
        "\n",
        "      ########################\n",
        "      return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HroHRZ1p79ep"
      },
      "source": [
        "We will train the new model with dropout to see its performance. The expected training time for the next experiment is around **12 minutes**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPgPmxF_8Ldp"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "weight_decay = 1e-3\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = MyMLPBNDrop(0.5) # We will use 0.5 probability to drop neurons out\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "train_losses, train_accuracies, dev_accuracies, dev_losses  = [], [], [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop_scheduler(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    dev_loss, dev_acc = test_loop(dev_dataloader, model, loss_fn, device)\n",
        "    dev_losses.append(dev_loss)\n",
        "    dev_accuracies.append(dev_acc)\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "\n",
        "print(f'Best dev accuracy: {max(dev_accuracies)} in epoch {dev_accuracies.index(max(dev_accuracies))+1}')\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t90PyHqrAE_w"
      },
      "source": [
        "Both train and dev accuracy are lower than before. More concretely, best dev accuracy is now 0.38, far from the 0.5 we got before. Again, my guess is that we should train much longer to see the effects of dropout better. We should also test different dropout rates and combination approaches with batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0859IvbsFWeb"
      },
      "source": [
        "##Performance on test data\n",
        "Even though our exploration has not been exhaustive, we obtained very interesting conclusions for this specific task:\n",
        "\n",
        "1.   One time setup considerations:\n",
        "\n",
        "\n",
        "*   Data prepocessing: scaled data (values between $[0, 1]$) works well.\n",
        "*   Weight initialization: He intialization is the best option.\n",
        "*   Batch normalization: use it!\n",
        "\n",
        "2.   Improve your training error.\n",
        "\n",
        "\n",
        "*   Optimizers: Adam with $\\alpha=10^{-4}$ works.\n",
        "*   Schedulers: using cosine scheduler we improve training results slightly.\n",
        "\n",
        "\n",
        "3.   Improve your test error.\n",
        "\n",
        "\n",
        "*   Data augmentation helps considerably.\n",
        "*   Weight decay offers slight improvements.\n",
        "*   Dropout does not help for this problem, although we haven't explored it enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BIAh98DK4dF"
      },
      "source": [
        "Let's train our final model, combining all those lessons and using **Early Stopping** for model selection. For Early Stopping, we have to make some decisions:\n",
        "\n",
        "\n",
        "1.   What performance indicator are we going to use to stop? In principle, we can choose between training loss, training accuracy, dev loss and dev accuracy. As we are trying to maximize dev accuracy, that will be our target metric.\n",
        "2.   How many epochs of patience will we allow? Patience is very important. Early stopping halts the training process when we do not improve a certain metric (dev accuracy, for example). However, patience allows to wait for some epochs before stopping. It could be the case where we do not improve for 4 epochs, but in the fifth one, we obtain a better result. If our patience is 0, we will not allow that kind of behaviours. So it is convenient to set a positive patience value, which is usually dependent on the total number of epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvYoOCm_Nadn"
      },
      "source": [
        "As this is the final training process, you can set a big number of epochs. Early stopping will halt the process if no improvements are observed!\n",
        "\n",
        "**EXERCISE:** Implement Early Stopping with patience. Basically, you have to implement an if-else statement. If current `dev_acc` is greater than the best value so far, store it and save the model to disk under the name of *model.pth*. Else, update the variable `epoch_no_improve` and if it is equal than `patience`, halt the process.\n",
        "\n",
        "**WARNING!** Long training process ahead! The following training process takes more than 40 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbAvi8oII3ch"
      },
      "source": [
        "import time\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "epochs = 150\n",
        "weight_decay = 1e-3\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = MyMLPBN()\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "# Early stopping stuff\n",
        "patience = 10\n",
        "best_dev_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "train_losses, train_accuracies, dev_accuracies, dev_losses  = [], [], [], []\n",
        "start = time.time()\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop_scheduler(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
        "    print(f'train loss: {train_loss}, train_acc: {train_acc}')\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    dev_loss, dev_acc = test_loop(dev_dataloader, model, loss_fn, device)\n",
        "    dev_losses.append(dev_loss)\n",
        "    dev_accuracies.append(dev_acc)\n",
        "\n",
        "    # Early stopping code\n",
        "    ### WRITE YOUR CODE HERE ### (≈ 9 lines)\n",
        "\n",
        "    ########################\n",
        "\n",
        "end = time.time()\n",
        "print(\"Done!\")\n",
        "\n",
        "print(f'Best dev accuracy: {max(dev_accuracies)} in epoch {dev_accuracies.index(max(dev_accuracies))+1}')\n",
        "print(f'Training time: {end-start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDGXPzIZA2dr"
      },
      "source": [
        "In my case, the experiment stopped at epoch 102. The best dev accuracy is 0.5763, obtained at epoch 92 (totally expected, as the patience value is 10).\n",
        "\n",
        "**HINT:** if you go to the file explorer in the left hand side of Colab, you can see the stored model *model.pth*. As Colab sessions are volatile, download the model to your local disk just in case. You can upload any file to your Colab disk in any moment.\n",
        "\n",
        "Let's visualize the learning curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31CgLMRVKaV5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(train_losses)), train_losses, color='blue', label='train')\n",
        "plt.plot(range(len(train_losses)), dev_losses, color='green', label='dev')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(train_accuracies)), train_accuracies, color='blue', label='train')\n",
        "plt.plot(range(len(train_accuracies)), dev_accuracies, color='green', label='dev')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SomtpPj1Bm9p"
      },
      "source": [
        "As said before, using regularization makes training more difficult, so the network needs more epochs. We can observe that training and development metrics are going together, but the gap between both is getting bigger and bigger as epochs increase. In the final epochs, training continues its slow improvement. However, development no longer improves. Although we use regularization techniques, we cannot prevent overfitting completely. In any case, remember that our first unregularized model had 0.54 dev accuracy. The regularized one is above 0.57, so the improvement is significant. Probably, using a higher patience value, we could even improve a little bit more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrXNLl16P_WW"
      },
      "source": [
        "Now comes the most important part. Let's load the best model from our training process, stored in the disk and check its performance on test data. Test images have not been used during the design and development stages of our neural network, so they are acutally **new data**. The performance estimation obtained using those images will be a very good estimation of the real performance of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb3ylwnVQB5J"
      },
      "source": [
        "model = torch.load('model.pth', map_location=torch.device(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvlwgU9EQC0B"
      },
      "source": [
        "Finally, download the test set and see how our model performs!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzsyqXzAG7Xp"
      },
      "source": [
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "print(f'Number of images: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nMSXbfuQMUC"
      },
      "source": [
        "batch_size = 64\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_loss, test_acc = test_loop(test_dataloader, model, loss_fn, device)\n",
        "\n",
        "print(f'test loss: {test_loss}, test accuracy: {test_acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7at3tli0QXLB"
      },
      "source": [
        "**This is our final result: 0.5837**. Our model can correctly classify almost 6 out of 10 images. Is this good enough? It depends on the application we pursue. But to have an idea, why don't you try it yourself?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rWYvhLhJS4v"
      },
      "source": [
        "# Let's visualize some images\n",
        "test_dataloader = DataLoader(test_data, batch_size=10, shuffle=True)\n",
        "imgs, labels = next(iter(test_dataloader)) # We have a batch of 10 images and labels\n",
        "\n",
        "# Make the predictions with our network\n",
        "model.eval()\n",
        "preds = model(imgs.to(device))\n",
        "pred_indices = preds.argmax(1).to('cpu').numpy()\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(imgs, nrow=10) # TODO: test this\n",
        "\n",
        "#imshow(out, title=[labels_map[x.item()] for x in labels])\n",
        "imshow(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29xvgZfWdd7j"
      },
      "source": [
        "Make your guess! You can right click the image above and open it in a new tab to zoom in and see each image better. Remember the possible labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhHcs-ecdobf"
      },
      "source": [
        "labels_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnZuCijpdvj4"
      },
      "source": [
        "Here you have the predictions of the network and the real labels. What about you? Are you better than our network?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_OpVpWgd182"
      },
      "source": [
        "pred_labels = [labels_map[x] for x in pred_indices]\n",
        "print(f'Predicted labels: {pred_labels}')\n",
        "gt_labels = [labels_map[x.item()] for x in labels]\n",
        "print(f'Real labels: {gt_labels}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UubGFQRFQdFw"
      },
      "source": [
        "**FINAL NOTES:**\n",
        "\n",
        "In this lab, we have seen that the techniques to improve the training performance work very well. Weight initialization, batch normalization, optimizers and schedulers helped us to reach 0.98 training accuracy with a neural network that initially was only able to reach 0.29.\n",
        "\n",
        "However, when we tried to improve our test performance, we were not *that* successful. The first model we tested in development data had an accuracy of 0.54. After exploring data augmentation, weight decay and dropout, we managed to achieve an accuracy above 0.57. Compared to how training evolved, testing improvement might seem quite poor. But there are several reasons for this:\n",
        "\n",
        "\n",
        "1.   Given the limited time of the labs and the relatively long training times required, we could not explore in depth all the regularization alternatives. More experiments regarding different transformations for data augmentation, weight decay values and dropout values and configurations could be run. Surely, we could improve even more our final performance.\n",
        "2.   Regularization is much more difficult than optimization. The real challenge in machine learning in general, and deep learning in particular, is **generalization**. Nowadays, optimization is better understood and stronger solutions have been proposed.\n",
        "3.   In this lab, we did not explore different neural network architectures. It is well known that MLPs overfit easily to high-dimensional input data, such as images. Even though we try much harder, we will always have the limits imposed by the real capacity of our MLP. Indeed, for images (and for other input types also) a different neural architecture is more suitable: **Convolutional Neural Networks**. And that is precisely the next topic of our course!\n",
        "\n"
      ]
    }
  ]
}